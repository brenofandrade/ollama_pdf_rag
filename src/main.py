import logging
import warnings
import tempfile
from pathlib import Path

import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


CHUNK_SIZE = 100
CHUNK_OVERLAP = 50
MODEL_EMBEDDING = "mxbai-embed-large"


def resetar_chat() -> None:
    st.session_state.messages = []


def carregar_documentos(arquivos):
    """Carrega e divide PDFs enviados pelo usuÃ¡rio."""
    documentos = []
    for arquivo in arquivos:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(arquivo.read())
            loader = PyPDFLoader(tmp.name)
            documentos.extend(loader.load())
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
    )
    return splitter.split_documents(documentos)


def construir_rag(docs):
    """Cria vetor de embeddings e cadeia RAG a partir dos documentos."""
    embeddings = OllamaEmbeddings(model=MODEL_EMBEDDING)
    vectorstore = Chroma.from_documents(docs, embeddings)
    prompt = ChatPromptTemplate.from_template(
        "Use os documentos a seguir para responder a pergunta.\n{documents}\nPergunta: {question}"
    )
    llm = ChatOllama(model="llama3.2:1b", temperature=0)
    rag_chain = prompt | llm | StrOutputParser()
    retriever = vectorstore.as_retriever()
    return retriever, rag_chain


def executar_pergunta(pergunta: str, retriever, rag_chain) -> str:
    docs = retriever.invoke(pergunta)
    textos = "\n".join(doc.page_content for doc in docs)
    return rag_chain.invoke({"question": pergunta, "documents": textos})


def main():
    warnings.filterwarnings("ignore", category=UserWarning)
    st.set_page_config(
        page_title="ğŸ“„ RAG com PDFs + Ollama",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    st.title("ğŸ“„ğŸ’¬ Assistente RAG para PDFs com Ollama + LangChain")
    st.caption(
        "Carregue documentos PDF, processe-os e faÃ§a perguntas sobre seu conteÃºdo com modelos locais via Ollama. "
        "Powered by LangChain ğŸ¦œğŸ”—"
    )
    st.markdown("---")

    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "retriever" not in st.session_state:
        st.session_state.retriever = None
        st.session_state.rag_chain = None

    col1, col2 = st.columns([1, 2])

    with col1:
        st.header("ğŸ“‚ Documentos", divider="gray")
        arquivos = st.file_uploader(
            "Envie um ou mais PDFs:",
            type="pdf",
            accept_multiple_files=True,
        )
        if st.button("Processar PDFs") and arquivos:
            docs = carregar_documentos(arquivos)
            retriever, rag_chain = construir_rag(docs)
            st.session_state.retriever = retriever
            st.session_state.rag_chain = rag_chain
            st.success("PDF(s) processado(s) com sucesso!")
        st.button("Limpar Conversa", on_click=resetar_chat)

    with col2:
        st.header("ğŸ’¬ Chat com o Assistente", divider="gray")

        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.write(msg["content"])

        pergunta = st.chat_input("Digite sua pergunta aqui...")
        if pergunta and st.session_state.retriever:
            st.session_state.messages.append({"role": "user", "content": pergunta})
            with st.chat_message("user"):
                st.write(pergunta)
            with st.chat_message("ai"):
                st.write("Estou processando sua pergunta...")
                try:
                    resposta = executar_pergunta(
                        pergunta, st.session_state.retriever, st.session_state.rag_chain
                    )
                except Exception as e:
                    resposta = f"Erro ao processar pergunta: {e}"
                st.write(resposta)
                st.session_state.messages.append({"role": "ai", "content": resposta})

    st.markdown("---")
    st.caption("Desenvolvido com â¤ï¸ por NÃºcleo de CiÃªncia de Dados â€¢ Unimed Blumenau")


if __name__ == "__main__":
    main()
