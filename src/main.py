import streamlit as st
import logging
import ollama
import warnings

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

warnings.filterwarnings('ignore', category=UserWarning)

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="üìÑ RAG com PDFs + Ollama",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded",
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

logger = logging.getLogger(__name__)


def extract_model_names(models_info):
    """
    Extrai os nomes dos modelos dispon√≠veis no Ollama.
    """
    logger.info("Extraindo nomes dos modelos do Ollama")
    try:
        if hasattr(models_info, "models"):
            return tuple(model.model for model in models_info.models)
        return tuple()
    except Exception as e:
        logger.error(f"Erro ao extrair modelos: {e}")
        return tuple()


def main():
    # T√≠tulo e descri√ß√£o
    st.title("üìÑüí¨ Assistente RAG para PDFs com Ollama + LangChain")
    st.caption(
        "Carregue documentos PDF, processe-os e fa√ßa perguntas sobre seu conte√∫do com modelos locais via Ollama. "
        "Powered by LangChain ü¶úüîó"
    )
    st.markdown("---")

    # Layout principal
    col1, col2 = st.columns([1, 2])

    #### Configura√ß√£o do Modelo
    with col1:
        st.header("‚öôÔ∏è Configura√ß√£o", divider="gray")
        models_info = ollama.list()
        available_models = extract_model_names(models_info)

        if available_models:
            selected_model = st.selectbox(
                "Selecione o modelo dispon√≠vel:",
                available_models,
                key="model_select"
            )
            st.success(f"Modelo selecionado: **{selected_model}**")
        else:
            st.error("Nenhum modelo Ollama encontrado no sistema.")

        # use_sample = st.toggle(
        #     "Usar PDF de exemplo",
        #     help="Usa um documento pr√©-carregado para demonstra√ß√£o."
        # )

    #### Upload de Documentos
    with col1:
        st.header("üìÇ Documentos", divider="gray")
        uploaded_files = st.file_uploader(
            "Envie um ou mais PDFs:",
            type="pdf",
            accept_multiple_files=True,
            key="pdf_upload"
        )
        if uploaded_files:
            st.info(f"{len(uploaded_files)} arquivo(s) carregado(s) com sucesso.")

    #### √Årea do Chat
    with col2:
        st.header("üí¨ Chat com o Assistente", divider="gray")

        chat_placeholder = st.empty()
        input_placeholder = st.chat_input("Digite sua pergunta aqui...")

        if input_placeholder:
            with chat_placeholder.chat_message("user"):
                st.write(input_placeholder)

            # Placeholder para resposta do assistente
            with chat_placeholder.chat_message("ai"):
                st.write("Estou processando sua pergunta...")

        else:
            st.info("Envie uma pergunta para come√ßar a conversar com o assistente.")

    st.markdown("---")
    st.caption(
        """
        Desenvolvido com ‚ù§Ô∏è por N√∫cleo de Ci√™ncia de Dados ‚Ä¢ Unimed Blumenau
        """
        )


if __name__ == "__main__":
    main()
